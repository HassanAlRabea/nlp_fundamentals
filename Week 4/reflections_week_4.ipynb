{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2536a22b",
   "metadata": {},
   "source": [
    "\n",
    "1. How does the precision of an n-gram tagger behave as you increase the value of n from one to k where k > 3 is the value of your choice (depending on the computing resources you have at hand). You are free to choose your own corpus (it does not have to be brown like in the examples).\n",
    "\n",
    "    - \"As n gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the sparse data problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval).\" (NLP Python book) So it will increase precision and we would have less false positives (so better tagging) but way lesswords will be tagged). We can see it here:\n",
    "    \n",
    "    uni = nltk.UnigramTagger(train_sents) # Testing bigram accuracy\n",
    "    print(uni.accuracy(test_sents))\n",
    "\n",
    "    bi = nltk.BigramTagger(train_sents) # Testing bigram accuracy\n",
    "    print(bi.accuracy(test_sents))\n",
    "\n",
    "    tri = nltk.TrigramTagger(train_sents) # Testing bigram accuracy\n",
    "    print(tri.accuracy(test_sents))\n",
    "\n",
    "    quad = nltk.NgramTagger(n=4, train=train_sents) # Testing bigram accuracy\n",
    "    print(quad.accuracy(test_sents))\n",
    "\n",
    "    octa = nltk.NgramTagger(n=9, train=train_sents) # Testing bigram accuracy\n",
    "    print(octa.accuracy(test_sents))\n",
    "\n",
    "    hund = nltk.NgramTagger(n=100, train=train_sents) # Testing bigram accuracy\n",
    "    print(hund.accuracy(test_sents))\n",
    "\n",
    "    Accuracy results:\n",
    "    \n",
    "    uni = 0.8121200039868434\n",
    "    \n",
    "    bi = 0.10206319146815508\n",
    "    \n",
    "    tri = 0.0626931127279976\n",
    "    \n",
    "    quad = 0.05511811023622047\n",
    "    \n",
    "    octa = 0.05372271504036679\n",
    "    \n",
    "    hund = 0.05372271504036679\n",
    "\n",
    "\n",
    "2. What is the effect on that precision when sentence breaks are taken into account versus when they are ignored? (See the section Tagging Across Sentence Boundaries in the (Python textbook, Chapter 5).\n",
    "\n",
    "    - NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, tn-1 and preceding tags are set to None. If they were to consider context that crosses this sentency boundary, it would drop the precision of our results and increasing false postives. This is because the lexical category that closed the previous sentence has no bearing on the one that begins the next sentence.\n",
    "\n",
    "    \n",
    "3. Compare the accuracy and the training time of a non-transformer tagger to a trans- former tagger (each one of your own choice) on at least three different corpora (again or your choice)\n",
    "\n",
    "    - Tranformer based models can take up to 500x longer to train than n-gram based combined tagger and only be half as accurate. The combined n-gram tagger achieved 84% accuracy and it took no more than 1 second to train, while the brill based one achieved 42% accuracy and took over 500 seconds to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f85ffa7",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "341390ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121200039868434\n",
      "0.10206319146815508\n",
      "0.0626931127279976\n",
      "0.05511811023622047\n",
      "0.05372271504036679\n",
      "0.05372271504036679\n"
     ]
    }
   ],
   "source": [
    "## N-gram implementation\n",
    "\n",
    "import nltk\n",
    "from nltk.tag import brill, brill_trainer\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "# Obtain data\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news') #Tagged sentences \n",
    "brown_sents = brown.sents(categories='news') #Untagged sentences\n",
    "\n",
    "#Determine the train/test split - setting training data at 90%\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "#Split the data of tagged sentences\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "uni = nltk.UnigramTagger(train_sents) # Testing bigram accuracy\n",
    "print(uni.accuracy(test_sents))\n",
    "\n",
    "bi = nltk.BigramTagger(train_sents) # Testing bigram accuracy\n",
    "print(bi.accuracy(test_sents))\n",
    "\n",
    "tri = nltk.TrigramTagger(train_sents) # Testing bigram accuracy\n",
    "print(tri.accuracy(test_sents))\n",
    "\n",
    "quad = nltk.NgramTagger(n=4, train=train_sents) # Testing bigram accuracy\n",
    "print(quad.accuracy(test_sents))\n",
    "\n",
    "octa = nltk.NgramTagger(n=9, train=train_sents) # Testing bigram accuracy\n",
    "print(octa.accuracy(test_sents))\n",
    "\n",
    "hund = nltk.NgramTagger(n=100, train=train_sents) # Testing bigram accuracy\n",
    "print(hund.accuracy(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7f82cd",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c7937243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('1', 'CD-HL'), ('.', '.-HL')], [('Introduction', 'NN-HL')], ...]\n"
     ]
    }
   ],
   "source": [
    "# Obtain data\n",
    "brown_tagged_sents = brown.tagged_sents(categories='learned') #Tagged sentences \n",
    "brown_sents = brown.sents(categories='learned') #Untagged sentences\n",
    "\n",
    "print(brown_tagged_sents)\n",
    "#Determine the train/test split - setting training data at 90%\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "#Split the data of tagged sentences\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bef5d47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 797 ms\n",
      "Wall time: 799 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8394620117327228"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Build out a combined n-gram tagger\n",
    "\n",
    "\n",
    "\n",
    "#Building the combined taggers \n",
    "t0 = nltk.DefaultTagger('NN') #Default tagger is everything will be a noun if bigram and unigram taggers fail\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0) # If bigram fails we use this one\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1) # Main tagger \n",
    "t3 = nltk.TrigramTagger(train_sents, backoff=t2) # Main tagger \n",
    "\n",
    "#Obtain accuracy score\n",
    "t3.accuracy(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5970bcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_add',\n",
       " '_get_root',\n",
       " '_init',\n",
       " '_para_block_reader',\n",
       " '_resolve',\n",
       " '_unload',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'categories',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'license',\n",
       " 'open',\n",
       " 'paras',\n",
       " 'raw',\n",
       " 'readme',\n",
       " 'sents',\n",
       " 'tagged_paras',\n",
       " 'tagged_sents',\n",
       " 'tagged_words',\n",
       " 'words']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_methods = [method_name for method_name in dir(brown)\n",
    "                  if callable(getattr(brown, method_name))]\n",
    "object_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eb363f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 24s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Build out a tranformer based model\n",
    "\n",
    "# specify a model to train\n",
    "init_tagger = DefaultTagger('NN') # a default starting point\n",
    "templates = nltk.tag.brill.nltkdemo18() # use the demo templates\n",
    "\n",
    "#init here will define the initial tagger and template the the brill trainer will use \n",
    "b_trainer = brill_trainer.BrillTaggerTrainer(init_tagger, templates) # https://www.nltk.org/_modules/nltk/tag/brill_trainer.html\n",
    "\n",
    "#Determine the train/test split - setting training data at 90%\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "#Split the data of tagged sentences\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "\n",
    "max_r = 200 # just a few rules so that this will not take too long\n",
    "\n",
    "brill_tagger = b_trainer.train(train_sents, max_rules = max_r) # train the model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b1f60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed eval>:1: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 312 ms\n",
      "Wall time: 308 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.463013306624696"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "brill_tagger.evaluate(test_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a078c960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>n_runtime</th>\n",
       "      <th>n_accuracy</th>\n",
       "      <th>b_runtime</th>\n",
       "      <th>b_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brown - News</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841</td>\n",
       "      <td>526</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brown - Editorial</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.839</td>\n",
       "      <td>324</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dataset  n_runtime  n_accuracy  b_runtime  b_accuracy\n",
       "0       Brown - News        1.0       0.841        526        0.42\n",
       "1  Brown - Editorial        0.7       0.839        324        0.46"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "d = {'dataset': ['Brown - News', 'Brown - Editorial'], 'n_runtime': [1, 0.7], 'n_accuracy': [0.841, 0.839], 'b_runtime': [526, 324], 'b_accuracy': [0.42, 0.46]}\n",
    "results_df = pd.DataFrame(data=d)\n",
    "results_df\n",
    "# results_df.plot.bar(x='dataset', y = ['n_runtime', 'n_accuracy', 'b_runtime', 'b_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
