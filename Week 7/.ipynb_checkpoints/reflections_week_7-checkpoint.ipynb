{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1b5fb4",
   "metadata": {},
   "source": [
    "1. **How does the tf-idf weight distribution (computed for Question 2 on Week 2) differ between non-stemmed and stemmed versions of one same set of documents? Is there a visually identifiable difference (in their histograms)? Can a statistical test tell them apart?**\n",
    "\n",
    "    tf is the term frequency and idf is the inverse document frequency which is obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n",
    "\n",
    "    stemming will group all words which are derived from the same stem (ex: played, play,..etc.).  This grouping will increase the occurrence of this stem because frequencies are calculated using stem not words. So certain words will have higher frequencies and be present in more documents due to stemming hrouping them together.\n",
    "    Source: https://stackoverflow.com/questions/10464265/effects-of-stemming-on-the-term-frequency\n",
    "\n",
    "\n",
    "2. **What is the effect of the presence/absence of stemming in the proposed term/document similarity measures (Questions 2 & 3 of Week 2)? You can try it out and/or speculate about it on a conceptual level.**\n",
    "\n",
    "    It will increase similarity between documents. Documents that were already similar will increase in similarity due to the increased grouping & frequency of appearance of words due by stemming. Documents that were further apart and more ambigouous will also become more similar as differentiating words are stemmed and grouped together instead. So overall, stemming would reduce differentiation across documents.  \n",
    "\n",
    "\n",
    "3. **What happens to the proportion of words found in the sentiment lexicons (used on Week 3) when stemming or lemmatization is carried out versus when it is not? Do you expect this to affect NLP systems that seek to identify or quantify feelings?**\n",
    "\n",
    "    Stemming would reduce the noize further by grouping similar terms. Making it easier to determine the sentiment of the word and improving the overall results. It would also amplify the sentiment but increasing the frequency of ocurrences due to the grouping making it easier for a model to land on an answer. \n",
    "\n",
    "4. **What is the effect of using or not using stemming or lemmatization in the precision of n-gram taggers (Question 1 of Week 4)?**\n",
    "\n",
    "    It would increase the precision & performance of n-gram tagger as words would be simplified and complexity reduced. But I think it would give us just the wrogn tagging due to the changing nature of the word. (i.e walking to walk)\n",
    "    \n",
    "    \n",
    "5. **How are the transformer-based taggers affected by stemming or lemmatization (Question 3 of Week 4)?**\n",
    "\n",
    "    Transformer-based taggers will be faster and more performant due to the reduction of dimensionality of the problem. However, it will lead to wrongful tagging since verbs/adjectives are effectively converted to nouns. So i'm not sure if it's useful to stemm when trying to tag the word.  \n",
    "    \n",
    "\n",
    "6. **In order to use a similarity quantifier (like the one of Question 2 of Week 5) to detect suspected plagiarism, would you expect this to be more useful for this purpose or less so if lemmatization or stemming were used explicitly before querying for WordNet similarities?**\n",
    "\n",
    "    I think it would help overall. By stemming, we would improve the performance of the algorithm and reduce the problem complexity. Especially with lemmatization, if we bring words back to their roots, we are bound to catch synonyms better, and thus plagirism too. \n",
    "    \n",
    "\n",
    "7. **Would text prediction (like the auto-complete of Question 2 of Week 6) be easier in some way if stemming or lemmatization were included in some part of the process?**\n",
    "\n",
    "    No it wouldn't as it would lead to more gibberish. training a model on the complete data would probably improve predictions (but of course increase the # of features the model has to work with). \n",
    "    \n",
    "\n",
    "8. **Are there languages that are harder or easier to stem than English? What linguistic structures affect this?**\n",
    "\n",
    "    I would assume so dpending on the amount of rules that are available in each language. To stem well, we would need to figure out how to handle plurality and verbs amongst other things. Combined with sentence structure rules, stemming could potentially cause more harm and not be as useful. \n",
    "    \n",
    "\n",
    "9. **Are there languages in which even segmentation (splitting a paragraph into sentences and sentences into words) is challenging?**\n",
    "\n",
    "\n",
    "    Yes there especially languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited. (https://en.wikipedia.org/wiki/Text_segmentation) So you can't apply the same easy logic across the board - you need to account for these languages differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fa5c9",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22543048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the original Code used in that questionsb\n",
    "# Obtain the TF-IDF Weights\n",
    "# Importing the BBC News dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_raw = pd.read_csv('bbc-news-data.csv', sep='\\t')\n",
    "#Remove digits, we have 2224 articles\n",
    "vectorizer = TfidfVectorizer(token_pattern=r\"\\b[^\\d\\W]+\", stop_words = 'english', strip_accents='ascii')\n",
    "tf_idf = vectorizer.fit_transform(df_raw[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583c737a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>word</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61078470</th>\n",
       "      <td>2220</td>\n",
       "      <td>zvyagintsev</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61078471</th>\n",
       "      <td>2221</td>\n",
       "      <td>zvyagintsev</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61078472</th>\n",
       "      <td>2222</td>\n",
       "      <td>zvyagintsev</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61078473</th>\n",
       "      <td>2223</td>\n",
       "      <td>zvyagintsev</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61078474</th>\n",
       "      <td>2224</td>\n",
       "      <td>zvyagintsev</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61078475 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          level_0         word  tf_idf\n",
       "0               0           aa     0.0\n",
       "1               1           aa     0.0\n",
       "2               2           aa     0.0\n",
       "3               3           aa     0.0\n",
       "4               4           aa     0.0\n",
       "...           ...          ...     ...\n",
       "61078470     2220  zvyagintsev     0.0\n",
       "61078471     2221  zvyagintsev     0.0\n",
       "61078472     2222  zvyagintsev     0.0\n",
       "61078473     2223  zvyagintsev     0.0\n",
       "61078474     2224  zvyagintsev     0.0\n",
       "\n",
       "[61078475 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving results in a DF\n",
    "df_tfidf = pd.DataFrame(tf_idf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf = df_tfidf.reset_index(level=0)\n",
    "\n",
    "df_tfidf_melt = pd.melt(df_tfidf, id_vars=['level_0'], var_name='word', value_name='tf_idf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c015f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>word</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    level_0 word  tf_idf\n",
       "0         0   aa     0.0\n",
       "1         1   aa     0.0\n",
       "2         2   aa     0.0\n",
       "3         3   aa     0.0\n",
       "4         4   aa     0.0\n",
       "5         5   aa     0.0\n",
       "6         6   aa     0.0\n",
       "7         7   aa     0.0\n",
       "8         8   aa     0.0\n",
       "9         9   aa     0.0\n",
       "10       10   aa     0.0\n",
       "11       11   aa     0.0\n",
       "12       12   aa     0.0\n",
       "13       13   aa     0.0\n",
       "14       14   aa     0.0\n",
       "15       15   aa     0.0\n",
       "16       16   aa     0.0\n",
       "17       17   aa     0.0\n",
       "18       18   aa     0.0\n",
       "19       19   aa     0.0\n",
       "20       20   aa     0.0\n",
       "21       21   aa     0.0\n",
       "22       22   aa     0.0\n",
       "23       23   aa     0.0\n",
       "24       24   aa     0.0\n",
       "25       25   aa     0.0\n",
       "26       26   aa     0.0\n",
       "27       27   aa     0.0\n",
       "28       28   aa     0.0\n",
       "29       29   aa     0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf_melt.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
