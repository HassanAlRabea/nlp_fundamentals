{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb473a30",
   "metadata": {},
   "source": [
    "1. Combine a rule-based and a machine-learning chatbot (like those of the example code), using rules and training data of your choice (you may retain the example setup if you wish) to implement a bot that answers clear-cut situations based on rules and defaults to the responses from a fitted neural network when no rule matches. Interact with it and discuss the strengths and the weaknesses of your bot. Please include code snippets and an example dialogue in your response.\n",
    "\n",
    "\n",
    "    I tried to set this up but I couldn't for the life of me get to embed the ML portion in the rule based logic\n",
    "    Here is the trial code: \n",
    "    \n",
    "\n",
    "2. Does it sound like a good idea to continue training a chatbot based on the reactions that its responses elicit from the users in a live environment? What would you gain by doing this? What could go wrong? In what context could it make sense to do this?\n",
    "\n",
    "\n",
    "    No it doesn't for multiple reasons: It would make the chatbot slow for one, having to retrain after every response. It may work for a small number of users by leveraging high computing resources but it's not scalable and would incur high maintenance costs. It can also backfire if no guardrails are present, it means we can retrain the bot to the poitn where it degrades into something else. The reason for continuous training would be to improve the bot's understanding of the user - this is something that the google assistant does to improve the results it returns to the user. Learning about the user's favorite music, food, places, people, etc. makes the assitant more usefule, however google doesn't retrain live, but at specififc rate overtime.\n",
    "    \n",
    "\n",
    "3. How about training a chatbot in a completely non-supervised manner (meaning that no labeled data is ever used)? Does this sound feasible? What could cause difficulties with such an approach?\n",
    "\n",
    "    Yes it is quite feasible as supervised learning can become expensive. Chatbots require a lot of data, and if it all has to be labelled it would take a long time and money. But chatbots can perform intent detection & entity extraction in user utterances on their own using raw data. They can then construct a reply from training they received on conversational data. Difficulties will come up depending on the context of the chatbot. For example, if it's for medical purposes, the chatbot will need to be trained on medical data which may nto be readily available. One way to counter thsi would be to use pretrained models, and fine-tune it for the use case. \n",
    "    \n",
    "\n",
    "\n",
    "4. Find some chatbots (at least three) in the customer service options of the websites or social media of some companies or organizations that interest you. Test them out for a few minutes and write a brief review of each, speculating on how you imagine them to be implemented and if you can think of ways to improve them.\n",
    "\n",
    "\n",
    "    - www.searchunify.com: (an AI/ML website) They had a chatbot popup and started talking to me if I needed help. I asked it what can it do, and i was then directed to an agent in india. so... not very AI lol. He then tried to make me a customer. So it's a front for libe agent conneciton. \n",
    "    \n",
    "    - t-mobile.com: There is a chat that pops when browsing and asks if you need help. If you say something like internet plans the virtual assistant will engage and start asking you questions to fill the required slots. This bot is leveraging the popular rasa framework and works well since you ahve to select the replies you need at times. If you're not a customer however, or there is an issue, they'll connetc you to a human right away. \n",
    "    \n",
    "    \n",
    "    - tangerine.ca: Also leveraging the rasa framework where you create policies, slots and the bot will try to fill them and figure otu which policy to apply. So if you pick the answers, everythign is great. It's not really open to any intent because if you type in a custom response (ask about the weather or tell it that you are a monster from another dimension), the exception handling will ask you to write in soemthing else that it can recongnize. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bf0339",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce0b0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from nltk.chat.util import reflections\n",
    "from urllib.request import urlopen\n",
    "from random import random, shuffle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd60a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b552cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08d2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alrab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alrab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alrab\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set up the ML Portion\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/vishal-verma27/Building-a-Simple-Chatbot-in-Python-using-NLTK/611bc5c96f25aa0e5f8e71a97c421fab7781214e/Train_Bot.json'\n",
    "data = urlopen(url) \n",
    "intents = json.loads(data.read())\n",
    "nltk.download('punkt') # once per machine\n",
    "nltk.download('wordnet') # same\n",
    "nltk.download('omw-1.4') # same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d733ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nltk.WordNetLemmatizer()\n",
    "words = set()\n",
    "documents = list()\n",
    "classes = set()\n",
    "for i in intents['intents']:\n",
    "    t = i['tag']\n",
    "    classes.add(t)\n",
    "    for p in i['patterns']:\n",
    "        tokens = set([ l.lemmatize(w.lower()) for w in nltk.word_tokenize(p) ])\n",
    "        documents.append((tokens, t))\n",
    "        words |= tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9760ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9c5e64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [0, 0, 0, 0, 0, 1, 0, 0]\n",
      "2 [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "4 [0, 0, 0, 0, 1, 0, 0, 0]\n",
      "6 [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "1 [0, 1, 0, 0, 0, 0, 0, 0]\n",
      "5 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "2 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "2 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "3 [1, 0, 0, 0, 0, 0, 0, 0]\n",
      "8 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "2 [0, 0, 0, 0, 0, 1, 0, 0]\n",
      "1 [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "1 [0, 1, 0, 0, 0, 0, 0, 0]\n",
      "2 [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "2 [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "2 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "1 [0, 0, 0, 1, 0, 0, 0, 0]\n",
      "5 [0, 0, 0, 0, 0, 0, 1, 0]\n",
      "3 [0, 0, 1, 0, 0, 0, 0, 0]\n",
      "1 [0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Now we can make a training set\n",
    "words = sorted(list(words))\n",
    "classes = sorted(list(classes))\n",
    "training = []\n",
    "k = len(classes)\n",
    "os = 10\n",
    "for (pattern, tag) in documents:\n",
    "    # binary yes/no for which words appear in a pattern\n",
    "    label = [0] * k # a blank slate for one-hot\n",
    "    label[classes.index(tag)] = 1 # this is the desired class\n",
    "    BoW = [ 1 * (w in pattern) for w in words ] # 1 if word in pattern, 0 if not \n",
    "    for sample in range(os): # oversample by using subsets of the BoW\n",
    "      variant = [ (random() < 0.8) * b for b in BoW ]\n",
    "      if sum(variant) > 0:\n",
    "        training.append([variant, label])\n",
    "\n",
    "\n",
    "        \n",
    "#Random order\n",
    "shuffle(training)\n",
    "for t in range(10):\n",
    "  bag, tag = training[t]\n",
    "  print(sum(bag), tag)\n",
    "for t in range(10):\n",
    "  bag, tag = training[-t]\n",
    "  print(sum(bag), tag)\n",
    "\n",
    "# just going to use all of it, we are not just now interested in saving some for testing\n",
    "X = [ t[0] for t in training ]\n",
    "Y = [ t[1] for t in training ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4bd1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1211 158 8\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1211)              192549    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1211)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 605)               733260    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 605)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 605)               366630    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8)                 4848      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,297,287\n",
      "Trainable params: 1,297,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Setting up the model to train\n",
    "\n",
    "n = len(X)\n",
    "h = len(X[0])\n",
    "k = len(Y[0])\n",
    "print(n, h, k)\n",
    "m = Sequential()\n",
    "t = 0.5\n",
    "a = 'relu'\n",
    "m.add(Dense(n, input_shape = (h, ), activation = a))\n",
    "m.add(Dropout(t))\n",
    "m.add(Dense(n // 2, activation = a))\n",
    "m.add(Dropout(t))\n",
    "m.add(Dense(n // 2, activation = a))\n",
    "m.add(Dense(k, activation = 'softmax')) # just pick one class\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c654804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "122/122 [==============================] - 1s 3ms/step - loss: 2.0593 - categorical_accuracy: 0.1916\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 2.0197 - categorical_accuracy: 0.2254\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.9860 - categorical_accuracy: 0.2246\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.9668 - categorical_accuracy: 0.2230\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.9415 - categorical_accuracy: 0.2230\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.9241 - categorical_accuracy: 0.2238\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.9050 - categorical_accuracy: 0.2230\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 1.8870 - categorical_accuracy: 0.2287\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.8634 - categorical_accuracy: 0.2370\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.8425 - categorical_accuracy: 0.2552\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.8203 - categorical_accuracy: 0.2568\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.7963 - categorical_accuracy: 0.3361\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.7682 - categorical_accuracy: 0.3625\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.7401 - categorical_accuracy: 0.3947\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.7049 - categorical_accuracy: 0.4261\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.6671 - categorical_accuracy: 0.4690\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.6220 - categorical_accuracy: 0.4905\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.5870 - categorical_accuracy: 0.5145\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 1.5273 - categorical_accuracy: 0.5533\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.4724 - categorical_accuracy: 0.5681\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.4082 - categorical_accuracy: 0.6185\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.3569 - categorical_accuracy: 0.6301\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.2988 - categorical_accuracy: 0.6466\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.2186 - categorical_accuracy: 0.6862\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.1643 - categorical_accuracy: 0.6936\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.0917 - categorical_accuracy: 0.7333\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 1.0358 - categorical_accuracy: 0.7448\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.9701 - categorical_accuracy: 0.7754\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.9114 - categorical_accuracy: 0.7894\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.8480 - categorical_accuracy: 0.8101\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.7981 - categorical_accuracy: 0.8159\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.7368 - categorical_accuracy: 0.8439\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.7005 - categorical_accuracy: 0.8340\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.6562 - categorical_accuracy: 0.8613\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.6094 - categorical_accuracy: 0.8728\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.5693 - categorical_accuracy: 0.8852\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.5427 - categorical_accuracy: 0.8770\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4981 - categorical_accuracy: 0.8943\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4726 - categorical_accuracy: 0.8976\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4254 - categorical_accuracy: 0.9092\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.4113 - categorical_accuracy: 0.9207\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3880 - categorical_accuracy: 0.9216\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3606 - categorical_accuracy: 0.9273\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.3311 - categorical_accuracy: 0.9331\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.3157 - categorical_accuracy: 0.9364\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2918 - categorical_accuracy: 0.9472\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2669 - categorical_accuracy: 0.9546\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2644 - categorical_accuracy: 0.9513\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2495 - categorical_accuracy: 0.9513\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.2416 - categorical_accuracy: 0.9505\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2244 - categorical_accuracy: 0.9620\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2146 - categorical_accuracy: 0.9628\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.2116 - categorical_accuracy: 0.9538\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1989 - categorical_accuracy: 0.9579\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1941 - categorical_accuracy: 0.9612\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1891 - categorical_accuracy: 0.9529\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1793 - categorical_accuracy: 0.9620\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1721 - categorical_accuracy: 0.9604\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1608 - categorical_accuracy: 0.9661\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1540 - categorical_accuracy: 0.9653\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.1524 - categorical_accuracy: 0.9686\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1412 - categorical_accuracy: 0.9736\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1513 - categorical_accuracy: 0.9678\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1357 - categorical_accuracy: 0.9719\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1317 - categorical_accuracy: 0.9719\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1274 - categorical_accuracy: 0.9752\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1210 - categorical_accuracy: 0.9736\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1272 - categorical_accuracy: 0.9736\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1312 - categorical_accuracy: 0.9694\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1157 - categorical_accuracy: 0.9761\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1146 - categorical_accuracy: 0.9711\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1196 - categorical_accuracy: 0.9752\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.1127 - categorical_accuracy: 0.9818\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0985 - categorical_accuracy: 0.9785\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0948 - categorical_accuracy: 0.9769\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0981 - categorical_accuracy: 0.9827\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0988 - categorical_accuracy: 0.9785\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1031 - categorical_accuracy: 0.9769\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0925 - categorical_accuracy: 0.9761\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0955 - categorical_accuracy: 0.9752\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0829 - categorical_accuracy: 0.9802\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0892 - categorical_accuracy: 0.9810\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0840 - categorical_accuracy: 0.9827\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0863 - categorical_accuracy: 0.9761\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0794 - categorical_accuracy: 0.9827\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0815 - categorical_accuracy: 0.9810\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0916 - categorical_accuracy: 0.9752\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0697 - categorical_accuracy: 0.9818\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0695 - categorical_accuracy: 0.9876\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0808 - categorical_accuracy: 0.9818\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0730 - categorical_accuracy: 0.9843\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0790 - categorical_accuracy: 0.9777\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0741 - categorical_accuracy: 0.9818\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0760 - categorical_accuracy: 0.9777\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0727 - categorical_accuracy: 0.9785\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0739 - categorical_accuracy: 0.9810\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0652 - categorical_accuracy: 0.9827\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0733 - categorical_accuracy: 0.9818\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0643 - categorical_accuracy: 0.9843\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0667 - categorical_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "## Training the model\n",
    "\n",
    "m.compile(loss = 'categorical_crossentropy', \n",
    "          optimizer = SGD(learning_rate = 0.001, \n",
    "                          decay = 1e-5, \n",
    "                          momentum = 0.8, \n",
    "                          nesterov = True), \n",
    "          metrics = ['categorical_accuracy'])\n",
    "\n",
    "f = m.fit(np.array(X), np.array(Y), epochs = 100, batch_size = 10, verbose = 1) # just a little for a quick demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262db313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write here to what you want the bot to respond: hello\n"
     ]
    }
   ],
   "source": [
    "userinput = input('Write here to what you want the bot to respond: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64d6048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_default_response(userinput):\n",
    "    uw = set([ l.lemmatize(w) for w in nltk.word_tokenize(userinput) ])\n",
    "    BoW = [ 1 * (w in uw) for w in words ]\n",
    "#     print(len(BoW))\n",
    "    threshold = 0.3 # discard unpromising results\n",
    "    import tensorflow as tf\n",
    "    datapoint = tf.convert_to_tensor(np.array([BoW]))\n",
    "    p = m.predict(datapoint)[0] # predict just this one data point\n",
    "#     print(classes) # reminder\n",
    "#     for outcome, label in zip(p, classes):\n",
    "#         if outcome > threshold: # could be a match\n",
    "#             print(outcome, label)\n",
    "    best = np.argmax(p) # use the highest as our match\n",
    "    chosen = classes[best] # the class to use to respond\n",
    "#     print(f'Chose class {chosen}')\n",
    "    from random import choice # we pick one at random\n",
    "    for i in intents['intents']:\n",
    "        if i['tag'] == chosen: \n",
    "            print(choice(i['responses']))\n",
    "            break\n",
    "    return choice(i['responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0defd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "765f261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n",
      "Hello! how can i help you ?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        intro = ['Pleased to meet you, %1.', \"Hello, %1, how's it going?\",]\n",
    "        smalltalk = [ 'I am well, thanks', 'Okay',]\n",
    "        identify = ['I am a bot',]\n",
    "        positive = [ 'Nice', 'Excellent', 'Way to go', 'Good for you',]\n",
    "        negative = [ 'That sucks', 'Sorry to hear that', ':(',]\n",
    "        default = ml_default_response(\"Okay\")\n",
    "        \n",
    "        questionResponse = [ # will be matched in this order\n",
    "            [ r'my name is (.*)', intro ],\n",
    "            [ r'I am (.*)', intro ],\n",
    "            [ r'this is (.*)', intro ],\n",
    "            [ r'who am I talking to|who is this', identify ],\n",
    "            [ r\"how are you|how about you|what's up\", smalltalk ],\n",
    "            [ r'not too bad|fine|well|great', positive ],\n",
    "            [ r'bad|sucks', negative ],\n",
    "            [ r'bonjour|hi|whazzup (.*)', ['ohai', 'Bonjour, ça va?',] ], \n",
    "            [ r'sorry (.*)', ['no biggie',] ],\n",
    "            [ r'close|farewell (.*)', ['It was a pleasure', 'See you later',] ],\n",
    "            [ r'(.*)\\!', ['%1, for sure'] ],\n",
    "            [ r'(.*)\\?', ['whoah, %1!',] ],\n",
    "            [ r'(.*)', default ]\n",
    "        ]        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "338bd6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At your service, mate.\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Hello! how can i help you ?\n",
      ">Hello\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Hello! how can i help you ?\n",
      "n\n",
      ">Tetsing\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Hello! how can i help you ?\n",
      "l\n",
      ">aefaf\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Hello! how can i help you ?\n",
      " \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [92], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt your service, mate.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m chat \u001b[38;5;241m=\u001b[39m Chat( reflections)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [91], line 162\u001b[0m, in \u001b[0;36mChat.converse\u001b[1;34m(self, quit)\u001b[0m\n\u001b[0;32m    160\u001b[0m user_input \u001b[38;5;241m=\u001b[39m quit\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(user_input)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_fund\\lib\\site-packages\\ipykernel\\kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp_fund\\lib\\site-packages\\ipykernel\\kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "## Set up the input to the bot\n",
    "\n",
    "print('At your service, mate.')\n",
    "chat = Chat( reflections)\n",
    "chat.converse(quit = 'close') # typing close will make it stop expecting more messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "afa9df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "reflections = {\n",
    "    \"i am\": \"you are\",\n",
    "    \"i was\": \"you were\",\n",
    "    \"i\": \"you\",\n",
    "    \"i'm\": \"you are\",\n",
    "    \"i'd\": \"you would\",\n",
    "    \"i've\": \"you have\",\n",
    "    \"i'll\": \"you will\",\n",
    "    \"my\": \"your\",\n",
    "    \"you are\": \"I am\",\n",
    "    \"you were\": \"I was\",\n",
    "    \"you've\": \"I have\",\n",
    "    \"you'll\": \"I will\",\n",
    "    \"your\": \"my\",\n",
    "    \"yours\": \"mine\",\n",
    "    \"you\": \"me\",\n",
    "    \"me\": \"you\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class Chat:\n",
    "    def __init__(self, pairs, reflections={}):\n",
    "        \"\"\"\n",
    "        Initialize the chatbot.  Pairs is a list of patterns and responses.  Each\n",
    "        pattern is a regular expression matching the user's statement or question,\n",
    "        e.g. r'I like (.*)'.  For each such pattern a list of possible responses\n",
    "        is given, e.g. ['Why do you like %1', 'Did you ever dislike %1'].  Material\n",
    "        which is matched by parenthesized sections of the patterns (e.g. .*) is mapped to\n",
    "        the numbered positions in the responses, e.g. %1.\n",
    "\n",
    "        :type pairs: list of tuple\n",
    "        :param pairs: The patterns and responses\n",
    "        :type reflections: dict\n",
    "        :param reflections: A mapping between first and second person expressions\n",
    "        :rtype: None\n",
    "        \"\"\"\n",
    "        intro = ['Pleased to meet you, %1.', \"Hello, %1, how's it going?\",]\n",
    "        smalltalk = [ 'I am well, thanks', 'Okay',]\n",
    "        identify = ['I am a bot',]\n",
    "        positive = [ 'Nice', 'Excellent', 'Way to go', 'Good for you',]\n",
    "        negative = [ 'That sucks', 'Sorry to hear that', ':(',]\n",
    "        default = ml_default_response(\"Okay\")\n",
    "        \n",
    "        pairs = [ # will be matched in this order\n",
    "            [ r'my name is (.*)', intro ],\n",
    "            [ r'I am (.*)', intro ],\n",
    "            [ r'this is (.*)', intro ],\n",
    "            [ r'who am I talking to|who is this', identify ],\n",
    "            [ r\"how are you|how about you|what's up\", smalltalk ],\n",
    "            [ r'not too bad|fine|well|great', positive ],\n",
    "            [ r'bad|sucks', negative ],\n",
    "            [ r'bonjour|hi|whazzup (.*)', ['ohai', 'Bonjour, ça va?',] ], \n",
    "            [ r'sorry (.*)', ['no biggie',] ],\n",
    "            [ r'close|farewell (.*)', ['It was a pleasure', 'See you later',] ],\n",
    "            [ r'(.*)\\!', ['%1, for sure'] ],\n",
    "            [ r'(.*)\\?', ['whoah, %1!',] ],\n",
    "            [ r'(.*)', default ]\n",
    "        ]        \n",
    "        \n",
    "\n",
    "        _pairs = [(re.compile(x, re.IGNORECASE), y) for (x, y) in pairs]\n",
    "        self._reflections = reflections\n",
    "        self._regex = self._compile_reflections()\n",
    "\n",
    "\n",
    "    def _compile_reflections(self):\n",
    "        sorted_refl = sorted(self._reflections, key=len, reverse=True)\n",
    "        return re.compile(\n",
    "            r\"\\b({})\\b\".format(\"|\".join(map(re.escape, sorted_refl))), re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    def _substitute(self, str):\n",
    "        \"\"\"\n",
    "        Substitute words in the string, according to the specified reflections,\n",
    "        e.g. \"I'm\" -> \"you are\"\n",
    "\n",
    "        :type str: str\n",
    "        :param str: The string to be mapped\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "\n",
    "        return self._regex.sub(\n",
    "            lambda mo: self._reflections[mo.string[mo.start() : mo.end()]], str.lower()\n",
    "        )\n",
    "\n",
    "    def _wildcards(self, response, match):\n",
    "        pos = response.find(\"%\")\n",
    "        while pos >= 0:\n",
    "            num = int(response[pos + 1 : pos + 2])\n",
    "            response = (\n",
    "                response[:pos]\n",
    "                + self._substitute(match.group(num))\n",
    "                + response[pos + 2 :]\n",
    "            )\n",
    "            pos = response.find(\"%\")\n",
    "        return response\n",
    "\n",
    "    def respond(self, str):\n",
    "        \n",
    "        \n",
    "        intro = ['Pleased to meet you, %1.', \"Hello, %1, how's it going?\",]\n",
    "        smalltalk = [ 'I am well, thanks', 'Okay',]\n",
    "        identify = ['I am a bot',]\n",
    "        positive = [ 'Nice', 'Excellent', 'Way to go', 'Good for you',]\n",
    "        negative = [ 'That sucks', 'Sorry to hear that', ':(',]\n",
    "        default = ml_default_response(str)\n",
    "        \n",
    "        pairs = [ # will be matched in this order\n",
    "            [ r'my name is (.*)', intro ],\n",
    "            [ r'I am (.*)', intro ],\n",
    "            [ r'this is (.*)', intro ],\n",
    "            [ r'who am I talking to|who is this', identify ],\n",
    "            [ r\"how are you|how about you|what's up\", smalltalk ],\n",
    "            [ r'not too bad|fine|well|great', positive ],\n",
    "            [ r'bad|sucks', negative ],\n",
    "            [ r'bonjour|hi|whazzup (.*)', ['ohai', 'Bonjour, ça va?',] ], \n",
    "            [ r'sorry (.*)', ['no biggie',] ],\n",
    "            [ r'close|farewell (.*)', ['It was a pleasure', 'See you later',] ],\n",
    "            [ r'(.*)\\!', ['%1, for sure'] ],\n",
    "            [ r'(.*)\\?', ['whoah, %1!',] ],\n",
    "            [ r'(.*)', default ]\n",
    "        ]        \n",
    "        \n",
    "\n",
    "        _pairs = [(re.compile(x, re.IGNORECASE), y) for (x, y) in pairs]\n",
    "        \n",
    "        \"\"\"\n",
    "        Generate a response to the user input.\n",
    "\n",
    "        :type str: str\n",
    "        :param str: The string to be mapped\n",
    "        :rtype: str\n",
    "        \"\"\"\n",
    "\n",
    "        # check each pattern\n",
    "        for (pattern, response) in _pairs:\n",
    "            match = pattern.match(str)\n",
    "\n",
    "            # did the pattern match?\n",
    "            if match:\n",
    "                resp = random.choice(response)  # pick a random response\n",
    "                resp = self._wildcards(resp, match)  # process wildcards\n",
    "\n",
    "                # fix munged punctuation at the end\n",
    "                if resp[-2:] == \"?.\":\n",
    "                    resp = resp[:-2] + \".\"\n",
    "                if resp[-2:] == \"??\":\n",
    "                    resp = resp[:-2] + \"?\"\n",
    "                return resp\n",
    "\n",
    "\n",
    "    # Hold a conversation with a chatbot\n",
    "    def converse(self, quit=\"quit\"):\n",
    "        user_input = \"\"\n",
    "        while user_input != quit:\n",
    "            user_input = quit\n",
    "            try:\n",
    "                user_input = input(\">\")\n",
    "            except EOFError:\n",
    "                print(user_input)\n",
    "            if user_input:\n",
    "                while user_input[-1] in \"!.\":\n",
    "                    user_input = user_input[:-1]\n",
    "                print(self.respond(user_input))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
